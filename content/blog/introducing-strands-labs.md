---
title: "Introducing Strands Labs"
excerpt: "Exploring the Frontier of Agentic AI Development with Strands Labs"
publishedAt: "2026-02-24"
category: "AWS"
tags: []
author: "CloudNestle Team"
featured: false
---

## Exploring the Frontier of Agentic AI Development with Strands Labs

Welcome to a groundbreaking initiative that promises to revolutionize the way we develop and deploy agentic AI systems. We are thrilled to announce the launch of Strands Labs, a dedicated space for experimentation and innovation in the realm of agentic AI. Strands Labs is a new GitHub organization that will serve as a hub for cutting-edge projects, fostering a community of developers who are passionate about pushing the boundaries of what AI can achieve.

### Why Strands Labs?

The Strands Agents SDK, available in both Python and TypeScript, has already made a significant impact since its open-source release in May 2025. With over 14 million downloads, the SDK has proven to be a powerful tool for developers, enabling them to build sophisticated AI agents with ease. However, the rapid pace of innovation in AI requires a space where experimental ideas can be tested and refined without the constraints of a production release cycle. This is where Strands Labs comes in.

By separating Strands Labs from the main Strands SDK, we aim to create an environment that encourages rapid experimentation and learning. All development teams across Amazon are invited to contribute their innovative projects, allowing for a diverse range of ideas to be explored and shared with the community. Each project in Strands Labs will be accompanied by clear use cases, functional code, and tests, making it easier for developers to get started and contribute.

### Key Projects in Strands Labs

At launch, Strands Labs will feature three exciting projects: Robots, Robots Sim, and AI Functions. Let's delve into each of these projects to understand how they are shaping the future of agentic AI.

#### Strands Robots

Agentic AI is no longer confined to the digital world. With Strands Robots, we are exploring how AI agents can extend their capabilities to the physical world, interacting with and controlling robots in real-time. This project aims to democratize physical AI by providing simple APIs, open-source libraries, and managed services that allow developers to build coordinated agentic physical AI systems.

One of the key features of Strands Robots is its ability to connect AI agents to physical sensors and hardware through a unified interface. This enables rapid prototyping and algorithm development, even in the absence of physical robotic hardware. For example, we have integrated NVIDIA GR00T, a vision-language-action model, with Strands Agents to control robotic arms. This integration allows for sophisticated AI capabilities to be executed directly on embedded systems, such as the NVIDIA Jetson edge hardware.

Here's a practical example of how you can use the Strands Robot class to control a robotic arm:

```python
from strands import Agent
from strands_robots import Robot

# Create a robot with cameras
robot = Robot(
    tool_name="my_arm",
    robot="so101_follower",
    cameras={
        "front": {"type": "opencv", "index_or_path": "/dev/video0", "fps": 30},
        "wrist": {"type": "opencv", "index_or_path": "/dev/video2", "fps": 30}
    },
    port="/dev/ttyACM0",
    data_config="so100_dualcam"
)

# Create an agent with the robot tool
agent = Agent(tools=[robot])
agent("place the apple in the basket")
```

In this example, the Robot class running on edge devices can delegate complex reasoning to the cloud using large language models (LLMs) and other models when needed. This hybrid approach ensures that the system can handle both millisecond-level physical actions and deeper reasoning tasks.

#### Strands Robot Sim

While physical robots are incredible, they can be costly and time-consuming to set up. Strands Robot Sim addresses this challenge by providing a simulated environment for rapid prototyping and algorithm development. This simulation supports Libero benchmark environments, saac-GR00T VLA policy support via ZMQ, and an extensible interface for VLA providers.

The Strands Robot Simulation project offers two execution modes: full episode execution with final results and iterative control with visual feedback per batch. This modular design allows developers to swap policy implementations or simulation environments without restructuring core logic.

Here's an example of how to use the SimEnv class from strands_robots_sim to control simulated robots within Libero environments employing policies generated by the NVIDIA GR00T:

```python
import asyncio
import argparse

# Example assumes Libero is installed, GR00T inference service is operational on port 8000, and Docker with isaac-gr00t containers are accessible.
async def main():
    parser = argparse.ArgumentParser(description="Control simulated robots using NVIDIA GR00T policies.")
    parser.add_argument("--env", type=str, default="Libero", help="Simulation environment")
    args = parser.parse_args()

    from strands_robots_sim import SimEnv

    sim_env = SimEnv(env_name=args.env)
    await sim_env.run()

asyncio.run(main())
```

This simulation environment enables fast testing and iteration, making it an invaluable tool for developers looking to refine their agentic AI algorithms.

#### AI Functions

One of the challenges in AI development is bridging the gap between natural language specifications and executable code. AI Functions aims to address this by allowing developers to define agents using natural language specifications instead of code. Developers can write pre and post conditions in Python that validate behavior and generate working implementations.

This experiment is designed to narrow the trust gap when generating code with large language models (LLMs). By focusing developer time on validating their intentions, the framework handles the rest. This approach not only simplifies the development process but also enhances the reliability of the generated code.

### Real-World Use Cases and Best Practices

To truly understand the impact of Strands Labs, let's explore some real-world use cases and best practices for leveraging these projects.

#### Use Case: Autonomous Warehousing

Imagine an autonomous warehousing system where robots are responsible for picking, packing, and shipping products. Using Strands Robots, developers can build AI agents that control robotic arms to perform these tasks with precision. The integration with NVIDIA GR00T allows for real-time decision-making based on visual inputs, ensuring that the robots can adapt to changing environments and tasks.

By utilizing Strands Robot Sim, developers can prototype and test these agents in a simulated environment before deploying them in a real-world setting. This reduces the risk of errors and ensures that the agents are robust and reliable.

#### Best Practice: Iterative Development

One of the best practices we encourage is iterative development. Start with a simple prototype using Strands Robot Sim to validate your ideas. Once you have a working prototype, gradually introduce complexity by integrating physical robots using Strands Robots. This approach allows you to test and refine your agents in a controlled environment before full-scale deployment.

### Conclusion

Strands Labs represents a significant step forward in the world of agentic AI development. By providing a dedicated space for experimentation and innovation, we aim to foster a community of developers who are passionate about pushing the boundaries of what AI can achieve. Whether you are exploring how AI agents can interact with the physical world, prototyping in a simulated environment, or bridging the gap between natural language and code, Strands Labs has something to offer.

We invite you to join us on this exciting journey. Explore the projects, contribute your ideas, and be part of a community that is shaping the future of AI. Together, we can build a world where AI agents are not only intelligent but also capable of interacting with and transforming the physical world around us.

Happy developing!
